{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from functools import cache\n",
    "import pandas as pd\n",
    "import pyodbc\n",
    "import requests\n",
    "import heapq\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from math import floor\n",
    "from torch import nn\n",
    "from numpy import float32\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch rows from the database. Run once and only when needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the data in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8240287\n"
     ]
    }
   ],
   "source": [
    "df: pd.DataFrame = pd.read_parquet('bikeshare_data.parquet.gzip')  \n",
    "print(df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we fetch station_information, which will be used in later functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIKE_BASE_ENDPOINT = \"https://tor.publicbikesystem.net/ube/gbfs/v1/en/\" \n",
    "ENDPOINTS = {\n",
    "        \"station_status\": BIKE_BASE_ENDPOINT + \"station_status\",\n",
    "        \"station_information\": BIKE_BASE_ENDPOINT + \"station_information\",\n",
    "}\n",
    "\n",
    "\n",
    "def _fetch_station_status(endpoints: dict[str, str]) -> list[dict]:\n",
    "    \"\"\"Return a sorted list (by station_id) containing station status for all stations.\"\"\"\n",
    "    response = requests.get(endpoints[\"station_status\"])\n",
    "    response_data: dict = response.json()\n",
    "    return response_data[\"data\"][\"stations\"]\n",
    "\n",
    "def _fetch_station_information(endpoints: dict[str, str]) -> list[dict]:\n",
    "    \"\"\"Return a sorted list (by station_id) containing station information for all stations.\"\"\"\n",
    "    response = requests.get(endpoints[\"station_information\"])\n",
    "    response_data: dict = response.json()\n",
    "    return response_data[\"data\"][\"stations\"]\n",
    "\n",
    "station_status = _fetch_station_status(ENDPOINTS)\n",
    "station_information = _fetch_station_information(ENDPOINTS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the working station IDS, discarding EOL stations and other define ID related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_IDs() -> list[str]:\n",
    "    \"\"\"Return a sorted list with all the stationIDs.\"\"\"\n",
    "    ids = []\n",
    "    status_i = 0\n",
    "    information_i = 0\n",
    "    while status_i < len(station_status) and information_i < len(station_information):\n",
    "        status_station_id = int(station_status[status_i][\"station_id\"])\n",
    "        information_station_id = int(station_information[information_i][\"station_id\"])\n",
    "        if status_station_id < information_station_id:\n",
    "            status_i += 1\n",
    "        elif information_station_id < status_station_id:\n",
    "            information_i += 1\n",
    "        else:\n",
    "            ids.append(station_status[status_i][\"station_id\"])\n",
    "            status_i += 1\n",
    "            information_i += 1\n",
    "    \n",
    "    return ids\n",
    "\n",
    "original_station_ids = get_all_IDs()\n",
    "NUM_STATIONS_ORIGINAL = len(original_station_ids)\n",
    "\n",
    "\n",
    "# to be used for testing\n",
    "def id_to_original_ID(id: int) -> int:\n",
    "    \"\"\"Convert the id (internal) to the original ID in the data.\"\"\"\n",
    "    return original_station_ids[id]\n",
    "\n",
    "@cache\n",
    "def station_encoding(station_id: int) -> list[int]:\n",
    "    \"\"\"Return a one hot encoding of the original station ID, station_id, based on its index in original_station_ids.\"\"\"\n",
    "    one_hot_encoding = [0] * NUM_STATIONS_ORIGINAL\n",
    "    index = original_station_ids.index(station_id)\n",
    "    one_hot_encoding[index] = 1\n",
    "    return tuple(one_hot_encoding)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weather related code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of all weather status, taking from the open weather map website\n",
    "WEATHER_STATUSES = [\n",
    "        200, 201, 202, 210, 211, 212, 221, 230, 231, 232,\n",
    "        300, 301, 302, 310, 311, 312, 313, 314, 321,\n",
    "        500, 501, 502, 503, 504, 511, 520, 521, 522, 531,\n",
    "        600, 601, 602, 611, 612, 613, 615, 616, 620, 621, 622,\n",
    "        701, 711, 721, 731, 741, 751, 761, 762, 771, 781,\n",
    "        800,\n",
    "        801, 802, 803, 804\n",
    "]\n",
    "NUM_STATUSES =  len(WEATHER_STATUSES)\n",
    "\n",
    "@cache\n",
    "def weather_encoding(weather_status: int) -> tuple[int]:\n",
    "    \"\"\"Return a one hot encoding of weather_status based on its index in WEATHER_STATUSES.\"\"\"\n",
    "    one_hot_encoding = [0] * NUM_STATUSES\n",
    "    index = WEATHER_STATUSES.index(weather_status)\n",
    "    one_hot_encoding[index] = 1\n",
    "    return tuple(one_hot_encoding)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Certain stations are turned of for a day or some period of time, we ignore those stations in our analysis\n",
    "\n",
    "group_sizes = df.groupby(\"StationID\").size()\n",
    "size_to_groups: dict = group_sizes.groupby(group_sizes).apply(lambda x: list(x.index)).to_dict()\n",
    "\n",
    "station_ids = size_to_groups[max(size_to_groups.keys())]\n",
    "NUM_STATIONS = len(station_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['StationID', 'Time', 'NumBikes', 'NumDocks', 'Latitude', 'Longitude',\n",
      "       'Temperature', 'WeatherStatus'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "station_ids_set = set(station_ids)\n",
    "\n",
    "def nearest_k_stations(K: int) -> dict[int, list[int]]:\n",
    "    \"\"\"Compute the K nearest stations to all stations, not including itself.\n",
    "    \n",
    "    The returned dict is a mapping from (transformed) stationID to a list containing the \n",
    "    (transformed) station IDs of the nearest K stations.\n",
    "    \"\"\"\n",
    "    ids = []\n",
    "    coordinates = []\n",
    "    status_i = 0\n",
    "    information_i = 0\n",
    "    while status_i < len(station_status) and information_i < len(station_information):\n",
    "        status_station_id = int(station_status[status_i][\"station_id\"])\n",
    "        information_station_id = int(station_information[information_i][\"station_id\"])\n",
    "        if status_station_id < information_station_id:\n",
    "            status_i += 1\n",
    "        elif information_station_id < status_station_id:\n",
    "            information_i += 1\n",
    "        else:\n",
    "            if station_status[status_i][\"station_id\"] in station_ids_set:\n",
    "                # the tranformed stationID is the stationID used internally\n",
    "                # it is simply the index of the stationID in the original_station_ids list\n",
    "                transformed_station_id = len(ids)\n",
    "                ids.append(transformed_station_id)\n",
    "                coordinates.append((station_information[information_i][\"lat\"], station_information[information_i][\"lon\"]))\n",
    "            status_i += 1\n",
    "            information_i += 1\n",
    "    \n",
    "    k_nearest = {}\n",
    "    for i, id in enumerate(ids):\n",
    "        coordinate = coordinates[i]\n",
    "        distances = [(distance.euclidean(x, coordinate), j) for j, x in enumerate(coordinates)]\n",
    "        closest = heapq.nsmallest(K + 1, distances)\n",
    "        k_nearest[id] = [ids[j] for _, j in closest[1:]]\n",
    "    return k_nearest\n",
    "\n",
    "\n",
    "def nearest_k_stations_inclusive(K: int) -> dict[int, list[int]]:\n",
    "    \"\"\"Compute the K nearest stations to all stations, including itself.\n",
    "    \n",
    "    The returned dict is a mapping from (transformed) stationID to a list containing the \n",
    "    (transformed) station IDs of the nearest K stations.\n",
    "    \"\"\"\n",
    "    ids = []\n",
    "    coordinates = []\n",
    "    status_i = 0\n",
    "    information_i = 0\n",
    "    while status_i < len(station_status) and information_i < len(station_information):\n",
    "        status_station_id = int(station_status[status_i][\"station_id\"])\n",
    "        information_station_id = int(station_information[information_i][\"station_id\"])\n",
    "        if status_station_id < information_station_id:\n",
    "            status_i += 1\n",
    "        elif information_station_id < status_station_id:\n",
    "            information_i += 1\n",
    "        else:\n",
    "            if station_status[status_i][\"station_id\"] in station_ids_set:\n",
    "                # the tranformed stationID is the stationID used internally\n",
    "                # it is simply the index of the stationID in the original_station_ids list\n",
    "                transformed_station_id = len(ids)\n",
    "                ids.append(transformed_station_id)\n",
    "                coordinates.append((station_information[information_i][\"lat\"], station_information[information_i][\"lon\"]))\n",
    "            status_i += 1\n",
    "            information_i += 1\n",
    "    \n",
    "    k_nearest = {}\n",
    "    for i, id in enumerate(ids):\n",
    "        coordinate = coordinates[i]\n",
    "        distances = [(distance.euclidean(x, coordinate), j) for j, x in enumerate(coordinates)]\n",
    "        closest = heapq.nsmallest(K + 1, distances)\n",
    "        k_nearest[id] = [ids[j] for _, j in closest]\n",
    "    return k_nearest\n",
    "\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the data in the data frame. Further preprocessing will be done later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove station ids that are now removed from the endpoints, but were present when data collection started (1 station was problematic)\n",
    "df = df[df[\"StationID\"].isin(station_ids_set)]\n",
    "\n",
    "df = df.sort_values(by=[\"Time\", \"StationID\"])\n",
    "\n",
    "# df['StationID'] = df['StationID'].apply(station_encoding)\n",
    "# df['WeatherStatus'] = df['WeatherStatus'].apply(weather_encoding)\n",
    "\n",
    "# turn categorical variables into a one-hot encoding\n",
    "# df = pd.get_dummies(df, columns=[\"WeatherStatus\", \"StationID\"], drop_first=True)\n",
    "df = pd.get_dummies(df, columns=[\"WeatherStatus\",], drop_first=True)\n",
    "# df = df.drop('StationID', axis=1)\n",
    "\n",
    "# Extract year, month, day of the year, hours, and minutes\n",
    "df['year'] = df['Time'].dt.year\n",
    "df['month'] = df['Time'].dt.month\n",
    "df['dayoftheyear'] = df['Time'].dt.dayofyear\n",
    "df['hours'] = df['Time'].dt.hour + (df['Time'].dt.minute / 60)\n",
    "df['weekday'] = df['Time'].dt.weekday \n",
    "df = df.drop('Time', axis=1)\n",
    "\n",
    "# Copy the original NumBikes column for the target\n",
    "df['NumBikes_original'] = df['NumBikes'].copy()\n",
    "\n",
    "# maintain this list whenever a new numeric column is added\n",
    "numeric_cols = ['NumBikes', 'NumDocks', 'Latitude', 'Longitude', 'Temperature', 'year', 'month', 'dayoftheyear', 'hours', 'weekday']\n",
    "feature_scaler = StandardScaler()\n",
    "df[numeric_cols] = feature_scaler.fit_transform(df[numeric_cols])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['StationID', 'NumBikes', 'NumDocks', 'Latitude', 'Longitude',\n",
       "       'Temperature', 'WeatherStatus_701', 'WeatherStatus_741',\n",
       "       'WeatherStatus_800', 'WeatherStatus_801', 'WeatherStatus_802',\n",
       "       'WeatherStatus_803', 'WeatherStatus_804', 'year', 'month',\n",
       "       'dayoftheyear', 'hours', 'weekday', 'NumBikes_original'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns\n",
    "# TODO: check why this is different\n",
    "# df.head(820)\n",
    "# df.iloc[820][\"StationID\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train test validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for splitting\n",
    "TRAIN_RATIO = 0.70\n",
    "VALIDATION_RATIO = 0.15\n",
    "TEST_RATIO = 1 - TRAIN_RATIO - VALIDATION_RATIO\n",
    "\n",
    "# Ensure the ratios sum to 1\n",
    "assert TRAIN_RATIO + VALIDATION_RATIO + TEST_RATIO == 1.0\n",
    "\n",
    "# Calculate the indices for the splits\n",
    "n = len(df) // NUM_STATIONS\n",
    "train_end = floor(TRAIN_RATIO * n) * NUM_STATIONS  # First half for training\n",
    "val_end = floor((TRAIN_RATIO + VALIDATION_RATIO) * n) * NUM_STATIONS  # Next quarter for validation\n",
    "\n",
    "# Perform the split\n",
    "df_train = df.iloc[:train_end]\n",
    "df_val = df.iloc[train_end:val_end]\n",
    "df_test = df.iloc[val_end:]\n",
    "\n",
    "assert df_train.shape[0] + df_val.shape[0] + df_test.shape[0] == df.shape[0]\n",
    "assert df_train.shape[0] % NUM_STATIONS == 0\n",
    "assert df_val.shape[0] % NUM_STATIONS == 0 \n",
    "assert df_test.shape[0] % NUM_STATIONS == 0 \n",
    "# df_train, df_val, and df_test are your resulting splits\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the torch Dataset to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:  fix later (broken after normalization), Optimized version still works\n",
    "class BikeShareDataset(Dataset):  \n",
    "    def __init__(self, data: pd.DataFrame, lookback: int, lookback_size: int, horizon: int, k: int) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (pd.DataFrame): DataFrame containing time series data.\n",
    "            lookback (int): Number of time steps to look back.\n",
    "            lookback_size (int): Length of time to jump over each lookback\n",
    "            horizon (int): Number of time steps to predict.\n",
    "            k (int):: Number of nearest stations information to consider in predictions.\n",
    "        \"\"\"\n",
    "        self.FLAT_NUM_FEATURES = data.shape[1] * (k + 1)\n",
    "        self.data = data.to_numpy()\n",
    "        self.lookback = lookback\n",
    "        self.lookback_size = lookback_size\n",
    "        self.horizon = horizon\n",
    "        self.k = k\n",
    "        self.k_nearest_dict = nearest_k_stations(k)\n",
    "        self.k_nearest_differences = self._compute_k_nearest_differences()\n",
    "\n",
    "    def _compute_k_nearest_differences(self) -> dict[int, list[int]]:\n",
    "        \"\"\"Return the differences between the indices of neighbours in the pandas dataframe.\"\"\"\n",
    "        k_nearest_differences = defaultdict(list)\n",
    "        for station in self.k_nearest_dict:\n",
    "            for near_station in self.k_nearest_dict[station]:\n",
    "                diff = near_station - station\n",
    "                k_nearest_differences[station].append(diff) \n",
    "        return k_nearest_differences\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        FIRST_MINUTES_SKIPPED = self.lookback * self.lookback_size\n",
    "        padded_length = len(self.data) - (NUM_STATIONS * (self.horizon + FIRST_MINUTES_SKIPPED))\n",
    "        padded_length = max(padded_length, 0)\n",
    "        return padded_length\n",
    "            \n",
    "    def __getitem__(self, idx: int):\n",
    "        FIRST_MINUTES_SKIPPED = self.lookback * self.lookback_size\n",
    "        start_i = NUM_STATIONS * FIRST_MINUTES_SKIPPED\n",
    "        idx = idx + start_i     \n",
    "        current_station = idx % NUM_STATIONS\n",
    "        x = np.ndarray((self.lookback, self.k + 1, self.data.shape[1]))\n",
    "        for steps_behind in range(self.lookback):\n",
    "            for k in range(self.k + 1):\n",
    "                if k == 0:\n",
    "                    x[steps_behind][k] = self.data[idx - (steps_behind * self.lookback_size * NUM_STATIONS)]\n",
    "                else: \n",
    "                    x[steps_behind][k] = self.data[idx - (steps_behind * self.lookback_size * NUM_STATIONS) + self.k_nearest_differences[current_station][k - 1]]\n",
    "        x = x.reshape(self.lookback, self.FLAT_NUM_FEATURES)\n",
    "\n",
    "        y = np.ndarray((self.horizon))\n",
    "        for minutes_ahead in range(1, self.horizon + 1):\n",
    "            y[minutes_ahead - 1] = self.data[idx + (NUM_STATIONS * minutes_ahead)][0]  # index 0 is NumBikes\n",
    "        return x, y\n",
    "        \n",
    "\n",
    "class BikeShareDatasetOptimized(Dataset):\n",
    "    def __init__(self, data: pd.DataFrame, lookback: int, lookback_size: int, horizon: int, k: int) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (pd.DataFrame): DataFrame containing time series data.\n",
    "            lookback (int): Number of time steps to look back.\n",
    "            lookback_size (int): Length of time to jump over each lookback\n",
    "            horizon (int): Number of time steps to predict.\n",
    "            k (int):: Number of nearest stations information to consider in predictions.\n",
    "        \"\"\"\n",
    "        self.target_data = data['NumBikes_original'].to_numpy(dtype=float)\n",
    "        self.data = data.drop(columns=['NumBikes_original']).to_numpy(dtype=float)\n",
    "        self.FLAT_NUM_FEATURES = self.data.shape[1] * (k + 1)\n",
    "        self.lookback = lookback\n",
    "        self.lookback_size = lookback_size\n",
    "        self.horizon = horizon\n",
    "        self.k = k\n",
    "        self.k_nearest_dict = nearest_k_stations_inclusive(k)\n",
    "        self.k_nearest_differences = self._compute_k_nearest_differences()\n",
    "\n",
    "    def _compute_k_nearest_differences(self) -> dict[int, list[int]]:\n",
    "        \"\"\"Return the differences between the indices of neighbours in the pandas dataframe.\"\"\"\n",
    "        k_nearest_differences = defaultdict(np.array)\n",
    "        for station in self.k_nearest_dict:\n",
    "            near_stations =  np.array(self.k_nearest_dict[station])\n",
    "            diff = near_stations - station\n",
    "            k_nearest_differences[station] = diff\n",
    "            \n",
    "        return k_nearest_differences\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        FIRST_MINUTES_SKIPPED = self.lookback * self.lookback_size\n",
    "        padded_length = len(self.data) - (NUM_STATIONS * (self.horizon + FIRST_MINUTES_SKIPPED))\n",
    "        padded_length = max(padded_length, 0)\n",
    "        return padded_length\n",
    "            \n",
    "    def __getitem__(self, idx: int):\n",
    "        FIRST_MINUTES_SKIPPED = self.lookback * self.lookback_size\n",
    "        start_i = NUM_STATIONS * FIRST_MINUTES_SKIPPED\n",
    "        idx = idx + start_i     \n",
    "        current_station = idx % NUM_STATIONS\n",
    "        \n",
    "        indices = np.arange(idx, idx - (self.lookback - 1) * self.lookback_size * NUM_STATIONS - 1, -(self.lookback_size * NUM_STATIONS))\n",
    "        neighbour_diffs = self.k_nearest_differences[current_station]\n",
    "        x = self.data[indices[:, None] + neighbour_diffs[None, :]]\n",
    "        x = x.reshape(self.lookback, self.FLAT_NUM_FEATURES)\n",
    "        \n",
    "        current_occupancy = self.target_data[idx] # used for debugging\n",
    "\n",
    "        y_indices = idx + np.arange(1, self.horizon + 1) * NUM_STATIONS\n",
    "        y = self.target_data[y_indices] \n",
    "        return x, y, current_occupancy\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback = 6\n",
    "lookback_size = 5\n",
    "horizon = 120\n",
    "k = 5\n",
    "train_data = BikeShareDatasetOptimized(df_train, lookback, lookback_size, horizon, k)\n",
    "validation_data = BikeShareDatasetOptimized(df_val, lookback, lookback_size, horizon, k)\n",
    "test_data = BikeShareDatasetOptimized(df_test, lookback, lookback_size, horizon, k)\n",
    "\n",
    "batch_size = 124\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Timing Experiments on the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data = BikeShareDataset(df_train, lookback, lookback_size, horizon, k)\n",
    "# optimized_data = BikeShareDatasetOptimized(df_train, lookback, lookback_size, horizon, k)\n",
    "\n",
    "# def compare_get_item(indices: np.ndarray) -> None:\n",
    "#     total_time = 0\n",
    "#     total_time_optimized = 0\n",
    "\n",
    "#     for idx in indices:\n",
    "#         start = time.perf_counter()  # Use perf_counter for high-resolution timing\n",
    "#         X, y = data[idx]\n",
    "#         end = time.perf_counter()\n",
    "#         total_time += end - start\n",
    "\n",
    "#         start = time.perf_counter()  # Use perf_counter for high-resolution timing\n",
    "#         Xo, yo = optimized_data[idx]\n",
    "#         end = time.perf_counter()\n",
    "#         total_time_optimized += end - start\n",
    "        \n",
    "#         assert np.array_equal(X, Xo)\n",
    "#         assert np.array_equal(y, yo)\n",
    "\n",
    "#     print(f\"BikeShareDataset's __getitem took {total_time} seconds in total\")\n",
    "#     print(f\"BikeShareDatasetOptimized's __getitem took {total_time_optimized} seconds in total\")\n",
    "\n",
    "\n",
    "# random_indices = np.random.randint(low=0, high=len(data), size=10_000)\n",
    "# compare_get_item(random_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLAT_NUM_FEATURES = df.shape[1] * (k + 1)\n",
    "FLAT_NUM_FEATURES = (df.shape[1] - 1) * (k + 1)  # remove NumBikes_original\n",
    "\n",
    "class BikeShareLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size: int, horizon: int, lookback: int, num_layers: int):\n",
    "        super().__init__()\n",
    "        self.lookback = lookback\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.lstm = nn.LSTM(input_size=FLAT_NUM_FEATURES, \n",
    "                            hidden_size=hidden_size, \n",
    "                            num_layers=num_layers, \n",
    "                            batch_first=True\n",
    "        )\n",
    "        self.hidden_size = hidden_size\n",
    "        # self.linear = nn.Linear((hidden_size * self.lookback), horizon)\n",
    "        self.linear = nn.Linear(hidden_size, horizon)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        # x is of shape (batch_size, lookbacks, hidden_size)\n",
    "        x = self.linear(x[:, -1, :])\n",
    "        # x = x.reshape(batch_size, sequence_length * self.hidden_size)\n",
    "        # x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "class BikeShareNN(nn.Module):\n",
    "    def __init__(self, hidden_size: int, horizon: int, lookback: int):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.lookback = lookback\n",
    "        self.horizon = horizon\n",
    "        self.hidden_size = hidden_size\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(FLAT_NUM_FEATURES * lookback, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, self.horizon),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        assert x.shape[1] * x.shape[2] == FLAT_NUM_FEATURES * self.lookback\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class TrivialNN(nn.Module):\n",
    "    def __init__(self, horizon: int, lookback: int):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.lookback = lookback\n",
    "        self.horizon = horizon\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        result = torch.zeros((batch_size, self.horizon))\n",
    "        for i in range(batch_size):\n",
    "            current_occupancy = x[i, 0, 0]  # Assuming current occupancy is in the first column of features\n",
    "            result[i] = torch.zeros((self.horizon)) + current_occupancy\n",
    "        return result\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BikeShareNN(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=648, out_features=50, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=50, out_features=120, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "HIDDEN_SIZE = 50\n",
    "NUM_LAYERS = 3\n",
    "\n",
    "# model = BikeShareLSTM(HIDDEN_SIZE, horizon, lookback, NUM_LAYERS).to(device)\n",
    "model = BikeShareNN(HIDDEN_SIZE, horizon, lookback).to(device)\n",
    "# model = TrivialNN(horizon, lookback).to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y, current_occupancies) in enumerate(dataloader):\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        assert pred.shape == y.shape\n",
    "        # total_loss_by_time = torch.zeros((horizon), dtype=float)\n",
    "        loss = loss_fn(pred, y)\n",
    "        # total_loss_by_time += torch.sum(torch.abs(pred - y), dim=0)\n",
    "\n",
    "        total_delta_from_current_occupancy = torch.zeros((horizon), dtype=float)\n",
    "        for i, current_occupancy in enumerate(current_occupancies):\n",
    "            delta = pred[i] - current_occupancy  # Compute delta from current occupancy for each time step\n",
    "            total_delta_from_current_occupancy += torch.abs(delta)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            # average_loss_by_time = total_loss_by_time / (batch_size)\n",
    "            average_delta_from_current_occupancy = total_delta_from_current_occupancy / batch_size\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            # print(f\"average loss by time {average_loss_by_time}\")\n",
    "            print(f\"average delta from current occupancy {average_delta_from_current_occupancy}\")\n",
    "\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss =  0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    print(f\"Avg loss: {test_loss:>8f} \\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[254], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 33\u001b[0m     \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m     test_loop(validation_dataloader, model, loss_fn)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[252], line 8\u001b[0m, in \u001b[0;36mtrain_loop\u001b[1;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch, (X, y, current_occupancies) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[0;32m      7\u001b[0m     X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 8\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43my\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# Compute prediction and loss\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     pred \u001b[38;5;241m=\u001b[39m model(X)\n",
      "File \u001b[1;32mc:\\Users\\rraag\\development\\bikeshare-availibility-predictions\\prediction-model\\venv\\Lib\\site-packages\\debugpy\\_vendored\\pydevd\\_pydevd_bundle\\pydevd_frame.py:1197\u001b[0m, in \u001b[0;36mPyDBFrame.trace_dispatch\u001b[1;34m(self, frame, event, arg)\u001b[0m\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_line:\n\u001b[0;32m   1196\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_suspend(thread, step_cmd, original_step_cmd\u001b[38;5;241m=\u001b[39minfo\u001b[38;5;241m.\u001b[39mpydev_original_step_cmd)\n\u001b[1;32m-> 1197\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1198\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_return:  \u001b[38;5;66;03m# return event\u001b[39;00m\n\u001b[0;32m   1199\u001b[0m     back \u001b[38;5;241m=\u001b[39m frame\u001b[38;5;241m.\u001b[39mf_back\n",
      "File \u001b[1;32mc:\\Users\\rraag\\development\\bikeshare-availibility-predictions\\prediction-model\\venv\\Lib\\site-packages\\debugpy\\_vendored\\pydevd\\_pydevd_bundle\\pydevd_frame.py:165\u001b[0m, in \u001b[0;36mPyDBFrame.do_wait_suspend\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_wait_suspend\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 165\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rraag\\development\\bikeshare-availibility-predictions\\prediction-model\\venv\\Lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[0;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[0;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[1;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[0;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rraag\\development\\bikeshare-availibility-predictions\\prediction-model\\venv\\Lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[0;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[0;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[1;32m-> 2106\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[0;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "epochs = 5\n",
    "\n",
    "class WeightedL1Loss(nn.Module):\n",
    "    def __init__(self, weights):\n",
    "        super(WeightedL1Loss, self).__init__()\n",
    "        self.weights = weights\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        # Ensure the weights are on the same device as predictions and targets\n",
    "        weights = self.weights.to(predictions.device)\n",
    "        \n",
    "        # Calculate the L1 loss (element-wise absolute difference)\n",
    "        loss = torch.abs(predictions - targets)\n",
    "        \n",
    "        # Apply the weights to the loss components\n",
    "        weighted_loss = loss * weights\n",
    "        \n",
    "        # Return the mean of the weighted loss\n",
    "        return torch.mean(weighted_loss)\n",
    "\n",
    "\n",
    "# weights = torch.linspace(1, 0.1, steps=horizon)\n",
    "\n",
    "\n",
    "# loss_fn = nn.L1Loss(reduction=\"mean\")\n",
    "loss_fn = nn.L1Loss(reduction=\"mean\")\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(validation_dataloader, model, loss_fn)\n",
    "    \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
