{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from functools import cache\n",
    "import pandas as pd\n",
    "import pyodbc\n",
    "import requests\n",
    "import heapq\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from math import floor\n",
    "from torch import nn\n",
    "from numpy import float32\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch rows from the database. Run once and only when needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the data in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8240287\n"
     ]
    }
   ],
   "source": [
    "df: pd.DataFrame = pd.read_parquet('bikeshare_data.parquet.gzip')  \n",
    "print(df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we fetch station_information, which will be used in later functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIKE_BASE_ENDPOINT = \"https://tor.publicbikesystem.net/ube/gbfs/v1/en/\" \n",
    "ENDPOINTS = {\n",
    "        \"station_status\": BIKE_BASE_ENDPOINT + \"station_status\",\n",
    "        \"station_information\": BIKE_BASE_ENDPOINT + \"station_information\",\n",
    "}\n",
    "\n",
    "\n",
    "def _fetch_station_status(endpoints: dict[str, str]) -> list[dict]:\n",
    "    \"\"\"Return a sorted list (by station_id) containing station status for all stations.\"\"\"\n",
    "    response = requests.get(endpoints[\"station_status\"])\n",
    "    response_data: dict = response.json()\n",
    "    return response_data[\"data\"][\"stations\"]\n",
    "\n",
    "def _fetch_station_information(endpoints: dict[str, str]) -> list[dict]:\n",
    "    \"\"\"Return a sorted list (by station_id) containing station information for all stations.\"\"\"\n",
    "    response = requests.get(endpoints[\"station_information\"])\n",
    "    response_data: dict = response.json()\n",
    "    return response_data[\"data\"][\"stations\"]\n",
    "\n",
    "station_status = _fetch_station_status(ENDPOINTS)\n",
    "station_information = _fetch_station_information(ENDPOINTS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the working station IDS, discarding EOL stations and other define ID related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_IDs() -> list[str]:\n",
    "    \"\"\"Return a sorted list with all the stationIDs.\"\"\"\n",
    "    ids = []\n",
    "    status_i = 0\n",
    "    information_i = 0\n",
    "    while status_i < len(station_status) and information_i < len(station_information):\n",
    "        status_station_id = int(station_status[status_i][\"station_id\"])\n",
    "        information_station_id = int(station_information[information_i][\"station_id\"])\n",
    "        if status_station_id < information_station_id:\n",
    "            status_i += 1\n",
    "        elif information_station_id < status_station_id:\n",
    "            information_i += 1\n",
    "        else:\n",
    "            ids.append(station_status[status_i][\"station_id\"])\n",
    "            status_i += 1\n",
    "            information_i += 1\n",
    "    \n",
    "    return ids\n",
    "\n",
    "original_station_ids = get_all_IDs()\n",
    "NUM_STATIONS_ORIGINAL = len(original_station_ids)\n",
    "\n",
    "\n",
    "# to be used for testing\n",
    "def id_to_original_ID(id: int) -> int:\n",
    "    \"\"\"Convert the id (internal) to the original ID in the data.\"\"\"\n",
    "    return original_station_ids[id]\n",
    "\n",
    "@cache\n",
    "def station_encoding(station_id: int) -> list[int]:\n",
    "    \"\"\"Return a one hot encoding of the original station ID, station_id, based on its index in original_station_ids.\"\"\"\n",
    "    one_hot_encoding = [0] * NUM_STATIONS_ORIGINAL\n",
    "    index = original_station_ids.index(station_id)\n",
    "    one_hot_encoding[index] = 1\n",
    "    return tuple(one_hot_encoding)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weather related code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of all weather status, taking from the open weather map website\n",
    "WEATHER_STATUSES = [\n",
    "        200, 201, 202, 210, 211, 212, 221, 230, 231, 232,\n",
    "        300, 301, 302, 310, 311, 312, 313, 314, 321,\n",
    "        500, 501, 502, 503, 504, 511, 520, 521, 522, 531,\n",
    "        600, 601, 602, 611, 612, 613, 615, 616, 620, 621, 622,\n",
    "        701, 711, 721, 731, 741, 751, 761, 762, 771, 781,\n",
    "        800,\n",
    "        801, 802, 803, 804\n",
    "]\n",
    "NUM_STATUSES =  len(WEATHER_STATUSES)\n",
    "\n",
    "@cache\n",
    "def weather_encoding(weather_status: int) -> tuple[int]:\n",
    "    \"\"\"Return a one hot encoding of weather_status based on its index in WEATHER_STATUSES.\"\"\"\n",
    "    one_hot_encoding = [0] * NUM_STATUSES\n",
    "    index = WEATHER_STATUSES.index(weather_status)\n",
    "    one_hot_encoding[index] = 1\n",
    "    return tuple(one_hot_encoding)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Certain stations are turned of for a day or some period of time, we ignore those stations in our analysis\n",
    "\n",
    "group_sizes = df.groupby(\"StationID\").size()\n",
    "size_to_groups: dict = group_sizes.groupby(group_sizes).apply(lambda x: list(x.index)).to_dict()\n",
    "\n",
    "station_ids = size_to_groups[max(size_to_groups.keys())]\n",
    "NUM_STATIONS = len(station_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['StationID', 'Time', 'NumBikes', 'NumDocks', 'Latitude', 'Longitude',\n",
      "       'Temperature', 'WeatherStatus'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "station_ids_set = set(station_ids)\n",
    "\n",
    "def nearest_k_stations(K: int) -> dict[int, list[int]]:\n",
    "    \"\"\"Compute the K nearest stations to all stations, not including itself.\n",
    "    \n",
    "    The returned dict is a mapping from (transformed) stationID to a list containing the \n",
    "    (transformed) station IDs of the nearest K stations.\n",
    "    \"\"\"\n",
    "    ids = []\n",
    "    coordinates = []\n",
    "    status_i = 0\n",
    "    information_i = 0\n",
    "    while status_i < len(station_status) and information_i < len(station_information):\n",
    "        status_station_id = int(station_status[status_i][\"station_id\"])\n",
    "        information_station_id = int(station_information[information_i][\"station_id\"])\n",
    "        if status_station_id < information_station_id:\n",
    "            status_i += 1\n",
    "        elif information_station_id < status_station_id:\n",
    "            information_i += 1\n",
    "        else:\n",
    "            if station_status[status_i][\"station_id\"] in station_ids_set:\n",
    "                # the tranformed stationID is the stationID used internally\n",
    "                # it is simply the index of the stationID in the original_station_ids list\n",
    "                transformed_station_id = len(ids)\n",
    "                ids.append(transformed_station_id)\n",
    "                coordinates.append((station_information[information_i][\"lat\"], station_information[information_i][\"lon\"]))\n",
    "            status_i += 1\n",
    "            information_i += 1\n",
    "    \n",
    "    k_nearest = {}\n",
    "    for i, id in enumerate(ids):\n",
    "        coordinate = coordinates[i]\n",
    "        distances = [(distance.euclidean(x, coordinate), j) for j, x in enumerate(coordinates)]\n",
    "        closest = heapq.nsmallest(K + 1, distances)\n",
    "        k_nearest[id] = [ids[j] for _, j in closest[1:]]\n",
    "    return k_nearest\n",
    "\n",
    "\n",
    "def nearest_k_stations_inclusive(K: int) -> dict[int, list[int]]:\n",
    "    \"\"\"Compute the K nearest stations to all stations, including itself.\n",
    "    \n",
    "    The returned dict is a mapping from (transformed) stationID to a list containing the \n",
    "    (transformed) station IDs of the nearest K stations.\n",
    "    \"\"\"\n",
    "    ids = []\n",
    "    coordinates = []\n",
    "    status_i = 0\n",
    "    information_i = 0\n",
    "    while status_i < len(station_status) and information_i < len(station_information):\n",
    "        status_station_id = int(station_status[status_i][\"station_id\"])\n",
    "        information_station_id = int(station_information[information_i][\"station_id\"])\n",
    "        if status_station_id < information_station_id:\n",
    "            status_i += 1\n",
    "        elif information_station_id < status_station_id:\n",
    "            information_i += 1\n",
    "        else:\n",
    "            if station_status[status_i][\"station_id\"] in station_ids_set:\n",
    "                # the tranformed stationID is the stationID used internally\n",
    "                # it is simply the index of the stationID in the original_station_ids list\n",
    "                transformed_station_id = len(ids)\n",
    "                ids.append(transformed_station_id)\n",
    "                coordinates.append((station_information[information_i][\"lat\"], station_information[information_i][\"lon\"]))\n",
    "            status_i += 1\n",
    "            information_i += 1\n",
    "    \n",
    "    k_nearest = {}\n",
    "    for i, id in enumerate(ids):\n",
    "        coordinate = coordinates[i]\n",
    "        distances = [(distance.euclidean(x, coordinate), j) for j, x in enumerate(coordinates)]\n",
    "        closest = heapq.nsmallest(K + 1, distances)\n",
    "        k_nearest[id] = [ids[j] for _, j in closest]\n",
    "    return k_nearest\n",
    "\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the data in the data frame. Further preprocessing will be done later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove station ids that are now removed from the endpoints, but were present when data collection started (1 station was problematic)\n",
    "df = df[df[\"StationID\"].isin(station_ids_set)]\n",
    "\n",
    "df = df.sort_values(by=[\"Time\", \"StationID\"])\n",
    "\n",
    "# df['StationID'] = df['StationID'].apply(station_encoding)\n",
    "# df['WeatherStatus'] = df['WeatherStatus'].apply(weather_encoding)\n",
    "\n",
    "# turn categorical variables into a one-hot encoding\n",
    "# df = pd.get_dummies(df, columns=[\"WeatherStatus\", \"StationID\"], drop_first=True)\n",
    "df = pd.get_dummies(df, columns=[\"WeatherStatus\",], drop_first=True)\n",
    "df = df.drop('StationID', axis=1)\n",
    "\n",
    "# Extract year, month, day of the year, hours, and minutes\n",
    "df['year'] = df['Time'].dt.year\n",
    "df['month'] = df['Time'].dt.month\n",
    "df['dayoftheyear'] = df['Time'].dt.dayofyear\n",
    "df['hours'] = df['Time'].dt.hour + (df['Time'].dt.minute / 60)\n",
    "df['weekday'] = df['Time'].dt.weekday \n",
    "df = df.drop('Time', axis=1)\n",
    "\n",
    "# Copy the original NumBikes column for the target\n",
    "df['NumBikes_original'] = df['NumBikes'].copy()\n",
    "\n",
    "# maintain this list whenever a new numeric column is added\n",
    "numeric_cols = ['NumBikes', 'NumDocks', 'Latitude', 'Longitude', 'Temperature', 'year', 'month', 'dayoftheyear', 'hours', 'weekday']\n",
    "feature_scaler = StandardScaler()\n",
    "df[numeric_cols] = feature_scaler.fit_transform(df[numeric_cols])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['NumBikes', 'NumDocks', 'Latitude', 'Longitude', 'Temperature',\n",
       "       'WeatherStatus_701', 'WeatherStatus_741', 'WeatherStatus_800',\n",
       "       'WeatherStatus_801', 'WeatherStatus_802', 'WeatherStatus_803',\n",
       "       'WeatherStatus_804', 'year', 'month', 'dayoftheyear', 'hours',\n",
       "       'weekday', 'NumBikes_original'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns\n",
    "# TODO: check why this is different\n",
    "# df.head(820)\n",
    "# df.iloc[820][\"StationID\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train test validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for splitting\n",
    "TRAIN_RATIO = 0.70\n",
    "VALIDATION_RATIO = 0.15\n",
    "TEST_RATIO = 1 - TRAIN_RATIO - VALIDATION_RATIO\n",
    "\n",
    "# Ensure the ratios sum to 1\n",
    "assert TRAIN_RATIO + VALIDATION_RATIO + TEST_RATIO == 1.0\n",
    "\n",
    "# Calculate the indices for the splits\n",
    "n = len(df) // NUM_STATIONS\n",
    "train_end = floor(TRAIN_RATIO * n) * NUM_STATIONS  # First half for training\n",
    "val_end = floor((TRAIN_RATIO + VALIDATION_RATIO) * n) * NUM_STATIONS  # Next quarter for validation\n",
    "\n",
    "# Perform the split\n",
    "df_train = df.iloc[:train_end]\n",
    "df_val = df.iloc[train_end:val_end]\n",
    "df_test = df.iloc[val_end:]\n",
    "\n",
    "assert df_train.shape[0] + df_val.shape[0] + df_test.shape[0] == df.shape[0]\n",
    "assert df_train.shape[0] % NUM_STATIONS == 0\n",
    "assert df_val.shape[0] % NUM_STATIONS == 0 \n",
    "assert df_test.shape[0] % NUM_STATIONS == 0 \n",
    "# df_train, df_val, and df_test are your resulting splits\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the torch Dataset to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:  fix later (broken after normalization), Optimized version still works\n",
    "class BikeShareDataset(Dataset):  \n",
    "    def __init__(self, data: pd.DataFrame, lookback: int, lookback_size: int, horizon: int, k: int) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (pd.DataFrame): DataFrame containing time series data.\n",
    "            lookback (int): Number of time steps to look back.\n",
    "            lookback_size (int): Length of time to jump over each lookback\n",
    "            horizon (int): Number of time steps to predict.\n",
    "            k (int):: Number of nearest stations information to consider in predictions.\n",
    "        \"\"\"\n",
    "        self.FLAT_NUM_FEATURES = data.shape[1] * (k + 1)\n",
    "        self.data = data.to_numpy()\n",
    "        self.lookback = lookback\n",
    "        self.lookback_size = lookback_size\n",
    "        self.horizon = horizon\n",
    "        self.k = k\n",
    "        self.k_nearest_dict = nearest_k_stations(k)\n",
    "        self.k_nearest_differences = self._compute_k_nearest_differences()\n",
    "\n",
    "    def _compute_k_nearest_differences(self) -> dict[int, list[int]]:\n",
    "        \"\"\"Return the differences between the indices of neighbours in the pandas dataframe.\"\"\"\n",
    "        k_nearest_differences = defaultdict(list)\n",
    "        for station in self.k_nearest_dict:\n",
    "            for near_station in self.k_nearest_dict[station]:\n",
    "                diff = near_station - station\n",
    "                k_nearest_differences[station].append(diff) \n",
    "        return k_nearest_differences\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        FIRST_MINUTES_SKIPPED = self.lookback * self.lookback_size\n",
    "        padded_length = len(self.data) - (NUM_STATIONS * (self.horizon + FIRST_MINUTES_SKIPPED))\n",
    "        padded_length = max(padded_length, 0)\n",
    "        return padded_length\n",
    "            \n",
    "    def __getitem__(self, idx: int):\n",
    "        FIRST_MINUTES_SKIPPED = self.lookback * self.lookback_size\n",
    "        start_i = NUM_STATIONS * FIRST_MINUTES_SKIPPED\n",
    "        idx = idx + start_i     \n",
    "        current_station = idx % NUM_STATIONS\n",
    "        x = np.ndarray((self.lookback, self.k + 1, self.data.shape[1]))\n",
    "        for steps_behind in range(self.lookback):\n",
    "            for k in range(self.k + 1):\n",
    "                if k == 0:\n",
    "                    x[steps_behind][k] = self.data[idx - (steps_behind * self.lookback_size * NUM_STATIONS)]\n",
    "                else: \n",
    "                    x[steps_behind][k] = self.data[idx - (steps_behind * self.lookback_size * NUM_STATIONS) + self.k_nearest_differences[current_station][k - 1]]\n",
    "        x = x.reshape(self.lookback, self.FLAT_NUM_FEATURES)\n",
    "\n",
    "        y = np.ndarray((self.horizon))\n",
    "        for minutes_ahead in range(1, self.horizon + 1):\n",
    "            y[minutes_ahead - 1] = self.data[idx + (NUM_STATIONS * minutes_ahead)][0]  # index 0 is NumBikes\n",
    "        return x, y\n",
    "        \n",
    "\n",
    "class BikeShareDatasetOptimized(Dataset):\n",
    "    def __init__(self, data: pd.DataFrame, lookback: int, lookback_size: int, horizon: int, k: int) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (pd.DataFrame): DataFrame containing time series data.\n",
    "            lookback (int): Number of time steps to look back.\n",
    "            lookback_size (int): Length of time to jump over each lookback\n",
    "            horizon (int): Number of time steps to predict.\n",
    "            k (int):: Number of nearest stations information to consider in predictions.\n",
    "        \"\"\"\n",
    "        self.target_data = data['NumBikes_original'].to_numpy(dtype=float)\n",
    "        self.data = data.drop(columns=['NumBikes_original']).to_numpy(dtype=float)\n",
    "        self.FLAT_NUM_FEATURES = self.data.shape[1] * (k + 1)\n",
    "        self.lookback = lookback\n",
    "        self.lookback_size = lookback_size\n",
    "        self.horizon = horizon\n",
    "        self.k = k\n",
    "        self.k_nearest_dict = nearest_k_stations_inclusive(k)\n",
    "        self.k_nearest_differences = self._compute_k_nearest_differences()\n",
    "\n",
    "    def _compute_k_nearest_differences(self) -> dict[int, list[int]]:\n",
    "        \"\"\"Return the differences between the indices of neighbours in the pandas dataframe.\"\"\"\n",
    "        k_nearest_differences = defaultdict(np.array)\n",
    "        for station in self.k_nearest_dict:\n",
    "            near_stations =  np.array(self.k_nearest_dict[station])\n",
    "            diff = near_stations - station\n",
    "            k_nearest_differences[station] = diff\n",
    "            \n",
    "        return k_nearest_differences\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        FIRST_MINUTES_SKIPPED = self.lookback * self.lookback_size\n",
    "        padded_length = len(self.data) - (NUM_STATIONS * (self.horizon + FIRST_MINUTES_SKIPPED))\n",
    "        padded_length = max(padded_length, 0)\n",
    "        return padded_length\n",
    "            \n",
    "    def __getitem__(self, idx: int):\n",
    "        FIRST_MINUTES_SKIPPED = self.lookback * self.lookback_size\n",
    "        start_i = NUM_STATIONS * FIRST_MINUTES_SKIPPED\n",
    "        idx = idx + start_i     \n",
    "        current_station = idx % NUM_STATIONS\n",
    "        \n",
    "        indices = np.arange(idx, idx - (self.lookback - 1) * self.lookback_size * NUM_STATIONS - 1, -(self.lookback_size * NUM_STATIONS))\n",
    "        neighbour_diffs = self.k_nearest_differences[current_station]\n",
    "        x = self.data[indices[:, None] + neighbour_diffs[None, :]]\n",
    "        x = x.reshape(self.lookback, self.FLAT_NUM_FEATURES)\n",
    "        \n",
    "        current_occupancy = self.target_data[idx] # used for debugging\n",
    "\n",
    "        y_indices = idx + np.arange(1, self.horizon + 1) * NUM_STATIONS\n",
    "        y = self.target_data[y_indices] \n",
    "        return x, y, current_occupancy\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback = 6\n",
    "lookback_size = 5\n",
    "horizon = 120\n",
    "k = 5\n",
    "train_data = BikeShareDatasetOptimized(df_train, lookback, lookback_size, horizon, k)\n",
    "validation_data = BikeShareDatasetOptimized(df_val, lookback, lookback_size, horizon, k)\n",
    "test_data = BikeShareDatasetOptimized(df_test, lookback, lookback_size, horizon, k)\n",
    "\n",
    "batch_size = 124\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Timing Experiments on the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data = BikeShareDataset(df_train, lookback, lookback_size, horizon, k)\n",
    "# optimized_data = BikeShareDatasetOptimized(df_train, lookback, lookback_size, horizon, k)\n",
    "\n",
    "# def compare_get_item(indices: np.ndarray) -> None:\n",
    "#     total_time = 0\n",
    "#     total_time_optimized = 0\n",
    "\n",
    "#     for idx in indices:\n",
    "#         start = time.perf_counter()  # Use perf_counter for high-resolution timing\n",
    "#         X, y = data[idx]\n",
    "#         end = time.perf_counter()\n",
    "#         total_time += end - start\n",
    "\n",
    "#         start = time.perf_counter()  # Use perf_counter for high-resolution timing\n",
    "#         Xo, yo = optimized_data[idx]\n",
    "#         end = time.perf_counter()\n",
    "#         total_time_optimized += end - start\n",
    "        \n",
    "#         assert np.array_equal(X, Xo)\n",
    "#         assert np.array_equal(y, yo)\n",
    "\n",
    "#     print(f\"BikeShareDataset's __getitem took {total_time} seconds in total\")\n",
    "#     print(f\"BikeShareDatasetOptimized's __getitem took {total_time_optimized} seconds in total\")\n",
    "\n",
    "\n",
    "# random_indices = np.random.randint(low=0, high=len(data), size=10_000)\n",
    "# compare_get_item(random_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLAT_NUM_FEATURES = df.shape[1] * (k + 1)\n",
    "FLAT_NUM_FEATURES = (df.shape[1] - 1) * (k + 1)  # remove NumBikes_original\n",
    "\n",
    "class BikeShareLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size: int, horizon: int, lookback: int, num_layers: int):\n",
    "        super().__init__()\n",
    "        self.lookback = lookback\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.lstm = nn.LSTM(input_size=FLAT_NUM_FEATURES, \n",
    "                            hidden_size=hidden_size, \n",
    "                            num_layers=num_layers, \n",
    "                            batch_first=True\n",
    "        )\n",
    "        self.hidden_size = hidden_size\n",
    "        # self.linear = nn.Linear((hidden_size * self.lookback), horizon)\n",
    "        self.linear = nn.Linear(hidden_size, horizon)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        # x is of shape (batch_size, lookbacks, hidden_size)\n",
    "        x = self.linear(x[:, -1, :])\n",
    "        # x = x.reshape(batch_size, sequence_length * self.hidden_size)\n",
    "        # x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "class BikeShareNN(nn.Module):\n",
    "    def __init__(self, hidden_size: int, horizon: int, lookback: int):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.lookback = lookback\n",
    "        self.horizon = horizon\n",
    "        self.hidden_size = hidden_size\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(FLAT_NUM_FEATURES * lookback, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, self.horizon),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        assert x.shape[1] * x.shape[2] == FLAT_NUM_FEATURES * self.lookback\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BikeShareNN(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=612, out_features=50, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=50, out_features=120, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "HIDDEN_SIZE = 50\n",
    "NUM_LAYERS = 3\n",
    "\n",
    "# model = BikeShareLSTM(HIDDEN_SIZE, horizon, lookback, NUM_LAYERS).to(device)\n",
    "model = BikeShareNN(HIDDEN_SIZE, horizon, lookback).to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    total_delta_from_current_occupancy = torch.zeros((horizon), dtype=float, requires_grad=False).to(device)\n",
    "    total_loss_by_time = torch.zeros((horizon), dtype=float, requires_grad=False).to(device)\n",
    "\n",
    "    for batch, (X, y, current_occupancies) in enumerate(dataloader):\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X, current_occupancies)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # Compute quantities for logging/debugging\n",
    "        total_loss_by_time += torch.sum(torch.abs(pred - y), dim=0)\n",
    "\n",
    "        for i, current_occupancy in enumerate(current_occupancies):\n",
    "            delta = pred[i] - current_occupancy  # Compute delta from current occupancy for each time step\n",
    "            total_delta_from_current_occupancy += torch.abs(delta)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    average_loss_by_time = total_loss_by_time / (batch_size)\n",
    "    print(f\"average loss by time {average_loss_by_time}\")\n",
    "    average_delta_from_current_occupancy = total_delta_from_current_occupancy / batch_size\n",
    "    print(f\"average delta from current occupancy {average_delta_from_current_occupancy}\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss =  0\n",
    "    total_delta_from_current_occupancy = torch.zeros((horizon), dtype=float, requires_grad=False).to(device)\n",
    "    total_loss_by_time = torch.zeros((horizon), dtype=float, requires_grad=False).to(device)\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y, current_occupancies in dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            \n",
    "            # Compute quantities for logging/debugging\n",
    "            total_loss_by_time += torch.sum(torch.abs(pred - y), dim=0)\n",
    "\n",
    "            for i, current_occupancy in enumerate(current_occupancies):\n",
    "                delta = pred[i] - current_occupancy  # Compute delta from current occupancy for each time step\n",
    "                total_delta_from_current_occupancy += torch.abs(delta)\n",
    "\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    print(f\"Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "    average_delta_from_current_occupancy = total_delta_from_current_occupancy / batch_size\n",
    "    print(f\"average delta from current occupancy {average_delta_from_current_occupancy}\")\n",
    "    \n",
    "    average_loss_by_time = total_loss_by_time / (batch_size)\n",
    "    print(f\"average loss by time {average_loss_by_time}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "epochs = 5\n",
    "\n",
    "class WeightedL1Loss(nn.Module):\n",
    "    def __init__(self, weights):\n",
    "        super(WeightedL1Loss, self).__init__()\n",
    "        self.weights = weights\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        # Ensure the weights are on the same device as predictions and targets\n",
    "        weights = self.weights.to(predictions.device)\n",
    "        \n",
    "        # Calculate the L1 loss (element-wise absolute difference)\n",
    "        loss = torch.abs(predictions - targets)\n",
    "        \n",
    "        # Apply the weights to the loss components\n",
    "        weighted_loss = loss * weights\n",
    "        \n",
    "        # Return the mean of the weighted loss\n",
    "        return torch.mean(weighted_loss)\n",
    "\n",
    "\n",
    "# weights = torch.linspace(1, 0.1, steps=horizon)\n",
    "\n",
    "\n",
    "# loss_fn = nn.L1Loss(reduction=\"mean\")\n",
    "loss_fn = nn.L1Loss(reduction=\"mean\")\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a trivial model on the loss function for a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[375], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAvg loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>8f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Assuming you have initialized the validation_dataloader and loss_fn\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[43mbaseline_test_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalidation_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[375], line 7\u001b[0m, in \u001b[0;36mbaseline_test_loop\u001b[1;34m(dataloader, loss_fn)\u001b[0m\n\u001b[0;32m      4\u001b[0m test_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m----> 7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_occupancies\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rraag\\development\\bikeshare-availibility-predictions\\prediction-model\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\rraag\\development\\bikeshare-availibility-predictions\\prediction-model\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\rraag\\development\\bikeshare-availibility-predictions\\prediction-model\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rraag\\development\\bikeshare-availibility-predictions\\prediction-model\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:317\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    257\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    259\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rraag\\development\\bikeshare-availibility-predictions\\prediction-model\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:174\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    171\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\rraag\\development\\bikeshare-availibility-predictions\\prediction-model\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 142\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\Users\\rraag\\development\\bikeshare-availibility-predictions\\prediction-model\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:223\u001b[0m, in \u001b[0;36mcollate_numpy_array_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np_str_obj_array_pattern\u001b[38;5;241m.\u001b[39msearch(elem\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mstr) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem\u001b[38;5;241m.\u001b[39mdtype))\n\u001b[1;32m--> 223\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rraag\\development\\bikeshare-availibility-predictions\\prediction-model\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 142\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\Users\\rraag\\development\\bikeshare-availibility-predictions\\prediction-model\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:214\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    212\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    213\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def baseline_test_loop(dataloader, loss_fn):\n",
    "    model.eval()\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y, current_occupancies in dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            current_occupancies = current_occupancies.to(device)\n",
    "\n",
    "            # Directly create predictions by broadcasting\n",
    "            pred = current_occupancies[:, None].expand(-1, horizon)\n",
    "\n",
    "            # Calculate the loss\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    print(f\"Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "# Assuming you have initialized the validation_dataloader and loss_fn\n",
    "baseline_test_loop(validation_dataloader, loss_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the actual model through test and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(validation_dataloader, model, loss_fn)\n",
    "    \n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
